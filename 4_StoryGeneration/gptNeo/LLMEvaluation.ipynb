{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd2e0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "You are an expert story reviewer. Rate the following short fictional story on a scale of 1 to 5 based on,Evaluate the story below and provide JSON-formatted feedback with:\n",
    "\n",
    "1. Coherence ‚Äì Clear beginning, middle, and end.\n",
    "2. Creativity ‚Äì Original or interesting concept.\n",
    "3. Language Fluency ‚Äì Well-written and grammatically correct.\n",
    "\n",
    "Do NOT compare this to novels or professional literature. Treat it as a short story by a beginner writer or AI.\n",
    "\n",
    "Example Response:\n",
    "{{\n",
    "  \"rating\": 4,\n",
    "  \"justification\": \"Good keyword usage but could improve character development\",\n",
    "  \"missing_keywords\": [\"Grandma\"]\n",
    "}}\n",
    "\n",
    "Example:\n",
    "keywords: [\"Mathville\", \"puzzles\", \"riddles\", \"equations\", \"nature\"]\n",
    "script: [Narrator](neutral): In the small town of Mathville, lived two best friends, Algy and Sammy. They loved solving puzzles and riddles together. One sunny day, they found a mysterious note on our favorite park bench! It read, \\u201cSolve -1147 = -11*a - 1213 for \\u2018a\\u2019. What is the answer?\n",
    "[Sammy](surprised): Who wrote this? And why should we care?\n",
    "[Algy](calm): No worries, Sammy. I'll take it on. Let's get to work! First, we need to get \\u2018a\\u2019 alone. To do that, let\\u2019s add 1213 to both sides of the equation.\n",
    "[Sammy](curious): And then what?\n",
    "[Algy](excited): Next, we simplify both sides to make our work easier! And voila! Both sides become 66.\n",
    "[Sammy](confused): But wait, won\\u2019t negative divided by negative give us positive? Why would we want to use minus here?\n",
    "[Algy](smiling): Great question, Sammy! Yes, usually, negative divided by negative gives us positive. But when you divide a number by itself \\u2013 no matter its sign \\u2013 the result will always be 1 or, in this case, -1 since we have a negative number.\n",
    "[Narrator](calm): After dividing, they discovered that 'a' was equal to -6.\n",
    "[Sammy](amazed): Whoa! The numbers around us are changing!\n",
    "[Algy](surprised): What's going on? Trees are turning into formulas, flowers into equations... even birds are chirping out integers!\n",
    "[Narrator](neutral): As they were about to celebrate their success, this bewildering spectacle made everyone realize how deeply intertwined math was with nature.\\n\\n[Sammy](reflective): Sometimes, unveiling answers leads us to surprising realizations. Though things didn't go back to normal immediately, we learned an essential lesson: Every discovery brings new wonders worth exploring.\"\n",
    "Rating: 5\n",
    "Justification: The story is simple but creative, flows logically, and is grammatically correct.\n",
    "\n",
    "Now rate this story:\n",
    "keywords: {keywords}\n",
    "script: {story}\n",
    "\n",
    "Your JSON analysis:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b939a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Individual Ratings:\n",
      "ü§ñ GPT-2: {'rating': 4, 'justification': 'Effective use of most keywords, but character development could be stronger', 'missing_keywords': ['Grandma']}\n",
      "üß† BART: {'rating': 4, 'justification': 'Effective use of most keywords, but character development could be stronger', 'missing_keywords': ['Grandma']}\n",
      "üîß T5: {'rating': 3, 'justification': '‚ö†Ô∏è Could not parse output properly.'}\n",
      "\n",
      "‚úÖ Average Rating: 3.67\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- STORY SETUP ---\n",
    "keywords = [\"Friendly Ghost\", \"Scared Girl\", \"Graveyard\", \"Grandma\", \"Friends\"]\n",
    "story = \"\"\"\n",
    "[Narrator](neutral): Once upon a time, there was a friendly ghost...\n",
    "[Friendly Ghost](calm): I love playing around the graveyard.\n",
    "[Narrator](neutral): One day, he met a scared little girl...\n",
    "[Friendly Ghost](surprised): Ooh, you're so scared!\n",
    "[Scared Little Girl](neutral): I want to go home.\n",
    "[Friendly Ghost](friendly): I'll take you with me.\n",
    "[Narrator](neutral): Together, they explored the graveyard...\n",
    "[Friendly Ghost](neutral): Let's hide and play some more.\n",
    "[Narrator](neutral): After a while, the ghost took the girl home...\n",
    "[Friendly Ghost](calm): I'll stay by her side, just in case.\n",
    "[Narrator](neutral): From then on, the ghost would visit the graveyard with his friend...\n",
    "[Friendly Ghost](happy): We'll play together and have fun.\n",
    "\"\"\"\n",
    "\n",
    "def safe_generate(model, tokenizer, prompt, max_input_len=1024, max_output_len=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len, padding=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    input_ids = input_ids[:, :max_input_len]  # Ensure strict truncation\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=max_output_len, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def extract_json_from_output(text):\n",
    "    try:\n",
    "        # Find start and end of JSON\n",
    "        start = text.find('{')\n",
    "        end = text.find('}', start) + 1\n",
    "        return json.loads(text[start:end])\n",
    "    except Exception:\n",
    "        return {\"rating\": 3, \"justification\": \"‚ö†Ô∏è Could not parse output properly.\"}\n",
    "\n",
    "def evaluate_model(model_name, prompt, keywords):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name) if \"gpt2\" in model_name else AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    output = safe_generate(model, tokenizer, prompt)\n",
    "    parsed = extract_json_from_output(output)\n",
    "    parsed[\"missing_keywords\"] = check_missing_keywords(keywords, story)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "\n",
    "# --- UTILITIES ---\n",
    "def print_colored(title, content):\n",
    "    colors = {'blue': '\\033[94m', 'green': '\\033[92m', 'end': '\\033[0m'}\n",
    "    print(f\"\\n{colors['blue']}=== {title} ==={colors['end']}\")\n",
    "    print(content)\n",
    "    print(f\"{colors['blue']}==================={colors['end']}\\n\")\n",
    "\n",
    "def parse_json_response(text):\n",
    "    try:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}') + 1\n",
    "        if start == -1 or end == 0:\n",
    "            raise ValueError(\"No JSON found in response\")\n",
    "        return json.loads(text[start:end])\n",
    "    except Exception as e:\n",
    "        print(f\"JSON Parse Error: {str(e)}\")\n",
    "        return {\"rating\": 3, \"justification\": \"Response format error\"}\n",
    "\n",
    "# --- MODEL EVALUATORS ---\n",
    "def evaluate_with_model(prompt, model_name, model_type, is_seq2seq=False):\n",
    "    try:\n",
    "        # Model loading\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Handle padding token for models that don't have it\n",
    "        if not is_seq2seq and tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token  # Fix for GPT-2\n",
    "        \n",
    "        model_class = AutoModelForSeq2SeqLM if is_seq2seq else AutoModelForCausalLM\n",
    "        model = model_class.from_pretrained(model_name)\n",
    "        \n",
    "        # Tokenization\n",
    "        if is_seq2seq:\n",
    "            inputs = tokenizer(\"Evaluate: \" + prompt, return_tensors=\"pt\", \n",
    "                             max_length=1024, truncation=True, padding=True)\n",
    "        else:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", \n",
    "                             max_length=1024, truncation=True, padding=True)\n",
    "        \n",
    "        # Generation\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Use configured pad token\n",
    "        )\n",
    "        \n",
    "        # Decoding\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print_colored(f\"{model_type.upper()} RAW OUTPUT\", decoded_output)\n",
    "        \n",
    "        return parse_json_response(decoded_output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{model_type} Error: {str(e)}\")\n",
    "        return {\"rating\": 3, \"justification\": \"Model evaluation failed\"}\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "def main():\n",
    "    # Prepare prompt\n",
    "    prompt = FEW_SHOT_PROMPT.format(keywords=keywords, story=story.strip())\n",
    "    print_colored(\"EVALUATION PROMPT\", prompt)\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = {\n",
    "        \"GPT-2\": evaluate_with_model(prompt, \"gpt2\", \"gpt-2\"),\n",
    "        \"BART\": evaluate_with_model(prompt, \"facebook/bart-large\", \"bart\", is_seq2seq=True),\n",
    "        \"T5\": evaluate_with_model(prompt, \"t5-large\", \"t5\", is_seq2seq=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate average\n",
    "    ratings = [v['rating'] for v in results.values()]\n",
    "    average = sum(ratings) / len(ratings)\n",
    "    \n",
    "    # Display results\n",
    "    print_colored(\"EVALUATION RESULTS\", json.dumps(results, indent=2))\n",
    "    print_colored(\"FINAL SCORE\", f\"Average Rating: {average:.2f}/5.00\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c48567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
